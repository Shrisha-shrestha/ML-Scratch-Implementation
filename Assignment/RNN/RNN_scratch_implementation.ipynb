{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "681b7cf6",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network (RNN) from Scratch\n",
    "\n",
    "## Project Overview\n",
    "This notebook implements a **time series forecasting** model using a Recurrent Neural Network built entirely from scratch. We use the **book_sales.csv** dataset to predict future book sales based on historical patterns.\n",
    "\n",
    "### Dataset\n",
    "- **Source**: book_sales.csv\n",
    "- **Features**: Date, Paperback sales, Hardcover sales\n",
    "- **Task**: Time series prediction of book sales\n",
    "\n",
    "### Why RNN for Time Series?\n",
    "Unlike traditional neural networks, RNNs can **remember previous information** through hidden states, making them perfect for sequential data. Each time step's output depends on both current input and previous hidden state, allowing the network to capture temporal dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c9720b",
   "metadata": {},
   "source": [
    "## Installation & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f3a1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449f3833",
   "metadata": {},
   "source": [
    "## Data Loading & Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47373f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the book sales dataset\n",
    "data = pd.read_csv(\"book_sales.csv\")\n",
    "print(\"Dataset Shape:\", data.shape)\n",
    "print(\"\\nFirst 10 rows:\")\n",
    "print(data.head(10))\n",
    "print(\"\\nDataset Info:\")\n",
    "print(data.info())\n",
    "print(\"\\nStatistical Summary:\")\n",
    "print(data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b5937e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the raw data\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot Paperback sales\n",
    "axes[0].plot(data.index, data['Paperback'], marker='o', linewidth=2, color='#1f77b4', markersize=4)\n",
    "axes[0].set_title('Paperback Sales Over Time', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Day')\n",
    "axes[0].set_ylabel('Sales')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot Hardcover sales\n",
    "axes[1].plot(data.index, data['Hardcover'], marker='s', linewidth=2, color='#ff7f0e', markersize=4)\n",
    "axes[1].set_title('Hardcover Sales Over Time', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Day')\n",
    "axes[1].set_ylabel('Sales')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3697aed4",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "### Sequence Preparation for RNN\n",
    "For time series prediction, we need to create sequences where:\n",
    "- **Input (X)**: Previous `time_steps` values\n",
    "- **Output (Y)**: Next value to predict\n",
    "\n",
    "Example with time_steps=3:\n",
    "- X = [day1, day2, day3] → Y = [day4]\n",
    "- X = [day2, day3, day4] → Y = [day5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554f64dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select target variable (we'll predict Paperback sales)\n",
    "sales_data = data['Paperback'].values.reshape(-1, 1)\n",
    "\n",
    "# Normalize the data to [0, 1] range\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = scaler.fit_transform(sales_data)\n",
    "\n",
    "print(\"Original data shape:\", sales_data.shape)\n",
    "print(\"Scaled data range: [{:.3f}, {:.3f}]\".format(scaled_data.min(), scaled_data.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea3120b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data, time_steps=5):\n",
    "    \"\"\"\n",
    "    Create sequences for time series prediction.\n",
    "    \n",
    "    Parameters:\n",
    "    - data: Normalized time series data\n",
    "    - time_steps: Number of previous time steps to use as input\n",
    "    \n",
    "    Returns:\n",
    "    - X: Input sequences (samples, time_steps, features)\n",
    "    - Y: Target values (samples, 1)\n",
    "    \"\"\"\n",
    "    X, Y = [], []\n",
    "    for i in range(len(data) - time_steps):\n",
    "        X.append(data[i:i + time_steps])\n",
    "        Y.append(data[i + time_steps])\n",
    "    \n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "    return X, Y\n",
    "\n",
    "# Create sequences with 5 time steps\n",
    "time_steps = 5\n",
    "X, Y = create_sequences(scaled_data, time_steps)\n",
    "\n",
    "print(\"Sequence shapes:\")\n",
    "print(f\"X (input sequences): {X.shape}\")  # (samples, time_steps, features)\n",
    "print(f\"Y (targets): {Y.shape}\")         # (samples, 1)\n",
    "\n",
    "# Split into train and test sets (80-20 split)\n",
    "train_size = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "Y_train, Y_test = Y[:train_size], Y[train_size:]\n",
    "\n",
    "print(f\"\\nTrain set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8877dc8",
   "metadata": {},
   "source": [
    "## RNN Implementation from Scratch\n",
    "\n",
    "### Architecture Overview\n",
    "Our RNN will have:\n",
    "1. **Input Layer**: Takes sequences of sales data\n",
    "2. **Hidden Layer 1**: First recurrent layer with tanh activation\n",
    "3. **Hidden Layer 2**: Second recurrent layer with tanh activation\n",
    "4. **Output Layer**: Linear layer for regression\n",
    "5. **Backpropagation Through Time (BPTT)**: Updates weights across all time steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa674fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    \"\"\"\n",
    "    Recurrent Neural Network with 2 hidden layers for time series prediction.\n",
    "    \n",
    "    Architecture:\n",
    "    - Input → Hidden Layer 1 (Recurrent) → Hidden Layer 2 (Recurrent) → Output\n",
    "    - Uses tanh activation for hidden layers\n",
    "    - Implements Backpropagation Through Time (BPTT)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size=1, hidden1_size=32, hidden2_size=16, output_size=1, learning_rate=0.001):\n",
    "        \"\"\"\n",
    "        Initialize RNN with random weights.\n",
    "        \n",
    "        Parameters:\n",
    "        - input_size: Dimension of input at each time step (default: 1)\n",
    "        - hidden1_size: Number of neurons in first hidden layer\n",
    "        - hidden2_size: Number of neurons in second hidden layer\n",
    "        - output_size: Dimension of output (default: 1)\n",
    "        - learning_rate: Learning rate for gradient descent\n",
    "        \"\"\"\n",
    "        self.input_size = input_size\n",
    "        self.hidden1_size = hidden1_size\n",
    "        self.hidden2_size = hidden2_size\n",
    "        self.output_size = output_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # Initialize weights for Hidden Layer 1\n",
    "        self.Wxh1 = np.random.randn(input_size, hidden1_size) * 0.01\n",
    "        self.Whh1 = np.random.randn(hidden1_size, hidden1_size) * 0.01\n",
    "        self.bh1 = np.zeros((1, hidden1_size))\n",
    "        \n",
    "        # Initialize weights for Hidden Layer 2\n",
    "        self.Wh1h2 = np.random.randn(hidden1_size, hidden2_size) * 0.01\n",
    "        self.Whh2 = np.random.randn(hidden2_size, hidden2_size) * 0.01\n",
    "        self.bh2 = np.zeros((1, hidden2_size))\n",
    "        \n",
    "        # Initialize weights for Output Layer\n",
    "        self.Why = np.random.randn(hidden2_size, output_size) * 0.01\n",
    "        self.by = np.zeros((1, output_size))\n",
    "        \n",
    "        # Track loss history\n",
    "        self.loss_history = []\n",
    "        \n",
    "    def tanh(self, x):\n",
    "        \"\"\"Hyperbolic tangent activation function\"\"\"\n",
    "        return np.tanh(x)\n",
    "    \n",
    "    def tanh_derivative(self, x):\n",
    "        \"\"\"Derivative of tanh for backpropagation\"\"\"\n",
    "        return 1 - np.tanh(x) ** 2\n",
    "    \n",
    "    def forward_propagation(self, X):\n",
    "        \"\"\"\n",
    "        Forward pass through the RNN for a single sequence.\n",
    "        \n",
    "        Parameters:\n",
    "        - X: Input sequence of shape (time_steps, input_size)\n",
    "        \n",
    "        Returns:\n",
    "        - y_pred: Predicted output\n",
    "        - cache: Dictionary containing intermediate values for backprop\n",
    "        \"\"\"\n",
    "        time_steps = X.shape[0]\n",
    "        \n",
    "        # Initialize hidden states\n",
    "        h1 = np.zeros((time_steps + 1, self.hidden1_size))\n",
    "        h2 = np.zeros((time_steps + 1, self.hidden2_size))\n",
    "        \n",
    "        # Store pre-activation values for backprop\n",
    "        z1 = np.zeros((time_steps, self.hidden1_size))\n",
    "        z2 = np.zeros((time_steps, self.hidden2_size))\n",
    "        \n",
    "        # Forward pass through time\n",
    "        for t in range(time_steps):\n",
    "            # Hidden Layer 1: h1[t] = tanh(Wxh1 * x[t] + Whh1 * h1[t-1] + bh1)\n",
    "            z1[t] = np.dot(X[t:t+1], self.Wxh1) + np.dot(h1[t:t+1], self.Whh1) + self.bh1\n",
    "            h1[t+1] = self.tanh(z1[t])\n",
    "            \n",
    "            # Hidden Layer 2: h2[t] = tanh(Wh1h2 * h1[t] + Whh2 * h2[t-1] + bh2)\n",
    "            z2[t] = np.dot(h1[t+1:t+2], self.Wh1h2) + np.dot(h2[t:t+1], self.Whh2) + self.bh2\n",
    "            h2[t+1] = self.tanh(z2[t])\n",
    "        \n",
    "        # Output: y = Why * h2[T] + by (use last hidden state)\n",
    "        y_pred = np.dot(h2[time_steps:time_steps+1], self.Why) + self.by\n",
    "        \n",
    "        # Cache for backpropagation\n",
    "        cache = {'X': X, 'h1': h1, 'h2': h2, 'z1': z1, 'z2': z2, 'y_pred': y_pred}\n",
    "        \n",
    "        return y_pred, cache\n",
    "    \n",
    "    def backward_propagation(self, cache, y_true):\n",
    "        \"\"\"\n",
    "        Backpropagation Through Time (BPTT).\n",
    "        \n",
    "        Parameters:\n",
    "        - cache: Dictionary from forward pass\n",
    "        - y_true: True target value\n",
    "        \n",
    "        Returns:\n",
    "        - gradients: Dictionary containing all weight gradients\n",
    "        \"\"\"\n",
    "        X = cache['X']\n",
    "        h1 = cache['h1']\n",
    "        h2 = cache['h2']\n",
    "        z1 = cache['z1']\n",
    "        z2 = cache['z2']\n",
    "        y_pred = cache['y_pred']\n",
    "        \n",
    "        time_steps = X.shape[0]\n",
    "        \n",
    "        # Initialize gradients\n",
    "        dWxh1 = np.zeros_like(self.Wxh1)\n",
    "        dWhh1 = np.zeros_like(self.Whh1)\n",
    "        dbh1 = np.zeros_like(self.bh1)\n",
    "        \n",
    "        dWh1h2 = np.zeros_like(self.Wh1h2)\n",
    "        dWhh2 = np.zeros_like(self.Whh2)\n",
    "        dbh2 = np.zeros_like(self.bh2)\n",
    "        \n",
    "        dWhy = np.zeros_like(self.Why)\n",
    "        dby = np.zeros_like(self.by)\n",
    "        \n",
    "        # Output layer gradient (MSE loss derivative)\n",
    "        dy = y_pred - y_true\n",
    "        \n",
    "        # Gradient for output weights\n",
    "        dWhy = np.dot(h2[time_steps:time_steps+1].T, dy)\n",
    "        dby = dy\n",
    "        \n",
    "        # Backpropagate to hidden layer 2\n",
    "        dh2_next = np.zeros((1, self.hidden2_size))\n",
    "        dh1_next = np.zeros((1, self.hidden1_size))\n",
    "        \n",
    "        # Gradient from output to last hidden state\n",
    "        dh2 = np.dot(dy, self.Why.T)\n",
    "        \n",
    "        # Backpropagate through time\n",
    "        for t in reversed(range(time_steps)):\n",
    "            # Add gradient from next time step\n",
    "            dh2 = dh2 + dh2_next\n",
    "            \n",
    "            # Gradient through tanh activation (hidden layer 2)\n",
    "            dz2 = dh2 * self.tanh_derivative(z2[t])\n",
    "            \n",
    "            # Accumulate gradients for hidden layer 2\n",
    "            dWh1h2 += np.dot(h1[t+1:t+2].T, dz2)\n",
    "            dWhh2 += np.dot(h2[t:t+1].T, dz2)\n",
    "            dbh2 += dz2\n",
    "            \n",
    "            # Gradient flowing to hidden layer 1\n",
    "            dh1 = np.dot(dz2, self.Wh1h2.T) + dh1_next\n",
    "            \n",
    "            # Gradient through tanh activation (hidden layer 1)\n",
    "            dz1 = dh1 * self.tanh_derivative(z1[t])\n",
    "            \n",
    "            # Accumulate gradients for hidden layer 1\n",
    "            dWxh1 += np.dot(X[t:t+1].T, dz1)\n",
    "            dWhh1 += np.dot(h1[t:t+1].T, dz1)\n",
    "            dbh1 += dz1\n",
    "            \n",
    "            # Prepare gradients for next time step (going backward)\n",
    "            dh2_next = np.dot(dz2, self.Whh2.T)\n",
    "            dh1_next = np.dot(dz1, self.Whh1.T)\n",
    "        \n",
    "        gradients = {\n",
    "            'dWxh1': dWxh1, 'dWhh1': dWhh1, 'dbh1': dbh1,\n",
    "            'dWh1h2': dWh1h2, 'dWhh2': dWhh2, 'dbh2': dbh2,\n",
    "            'dWhy': dWhy, 'dby': dby\n",
    "        }\n",
    "        \n",
    "        return gradients\n",
    "    \n",
    "    def update_weights(self, gradients):\n",
    "        \"\"\"Update weights using gradient descent.\"\"\"\n",
    "        # Clip gradients to prevent exploding gradients\n",
    "        max_grad = 5.0\n",
    "        for key in gradients:\n",
    "            np.clip(gradients[key], -max_grad, max_grad, out=gradients[key])\n",
    "        \n",
    "        # Update weights\n",
    "        self.Wxh1 -= self.learning_rate * gradients['dWxh1']\n",
    "        self.Whh1 -= self.learning_rate * gradients['dWhh1']\n",
    "        self.bh1 -= self.learning_rate * gradients['dbh1']\n",
    "        \n",
    "        self.Wh1h2 -= self.learning_rate * gradients['dWh1h2']\n",
    "        self.Whh2 -= self.learning_rate * gradients['dWhh2']\n",
    "        self.bh2 -= self.learning_rate * gradients['dbh2']\n",
    "        \n",
    "        self.Why -= self.learning_rate * gradients['dWhy']\n",
    "        self.by -= self.learning_rate * gradients['dby']\n",
    "    \n",
    "    def train(self, X_train, Y_train, epochs=100):\n",
    "        \"\"\"Train the RNN model.\"\"\"\n",
    "        num_samples = X_train.shape[0]\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            \n",
    "            # Train on each sequence\n",
    "            for i in range(num_samples):\n",
    "                # Forward pass\n",
    "                y_pred, cache = self.forward_propagation(X_train[i])\n",
    "                \n",
    "                # Compute loss (MSE)\n",
    "                loss = np.mean((y_pred - Y_train[i]) ** 2)\n",
    "                total_loss += loss\n",
    "                \n",
    "                # Backward pass\n",
    "                gradients = self.backward_propagation(cache, Y_train[i])\n",
    "                \n",
    "                # Update weights\n",
    "                self.update_weights(gradients)\n",
    "            \n",
    "            # Average loss for this epoch\n",
    "            avg_loss = total_loss / num_samples\n",
    "            self.loss_history.append(avg_loss)\n",
    "            \n",
    "            # Print progress\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                print(f\"Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.6f}\")\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions for input sequences.\"\"\"\n",
    "        predictions = []\n",
    "        for i in range(X.shape[0]):\n",
    "            y_pred, _ = self.forward_propagation(X[i])\n",
    "            predictions.append(y_pred[0, 0])\n",
    "        \n",
    "        return np.array(predictions).reshape(-1, 1)\n",
    "\n",
    "print(\"✓ RNN model with 2 hidden layers defined and ready for training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb925720",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25d068e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize RNN with 2 hidden layers\n",
    "print(\"Training RNN Model...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "model = RNN(\n",
    "    input_size=1,          # Single feature (sales value)\n",
    "    hidden1_size=32,       # First hidden layer with 32 neurons\n",
    "    hidden2_size=16,       # Second hidden layer with 16 neurons\n",
    "    output_size=1,         # Single output (predicted sales)\n",
    "    learning_rate=0.001    # Learning rate\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.train(X_train, Y_train, epochs=100)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"✓ Training Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5967ee72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(model.loss_history, linewidth=2, color='#d62728')\n",
    "plt.title('RNN Training Loss Over Epochs', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Mean Squared Error Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Initial Loss: {model.loss_history[0]:.6f}\")\n",
    "print(f\"Final Loss: {model.loss_history[-1]:.6f}\")\n",
    "print(f\"Loss Reduction: {((model.loss_history[0] - model.loss_history[-1]) / model.loss_history[0] * 100):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38017f83",
   "metadata": {},
   "source": [
    "## Model Evaluation & Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97028a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on train and test sets\n",
    "Y_train_pred = model.predict(X_train)\n",
    "Y_test_pred = model.predict(X_test)\n",
    "\n",
    "# Inverse transform to get actual sales values\n",
    "Y_train_actual = scaler.inverse_transform(Y_train)\n",
    "Y_train_pred_actual = scaler.inverse_transform(Y_train_pred)\n",
    "\n",
    "Y_test_actual = scaler.inverse_transform(Y_test)\n",
    "Y_test_pred_actual = scaler.inverse_transform(Y_test_pred)\n",
    "\n",
    "# Calculate evaluation metrics for test set\n",
    "mse = mean_squared_error(Y_test_actual, Y_test_pred_actual)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(Y_test_actual, Y_test_pred_actual)\n",
    "r2 = r2_score(Y_test_actual, Y_test_pred_actual)\n",
    "mape = np.mean(np.abs((Y_test_actual - Y_test_pred_actual) / Y_test_actual)) * 100\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TEST SET EVALUATION METRICS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Mean Squared Error (MSE):      {mse:.2f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
    "print(f\"Mean Absolute Error (MAE):      {mae:.2f}\")\n",
    "print(f\"R² Score:                       {r2:.4f}\")\n",
    "print(f\"Mean Absolute % Error (MAPE):   {mape:.2f}%\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650377e7",
   "metadata": {},
   "source": [
    "## Comprehensive Visualization of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd553fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "# 1. Train vs Predicted (Training Set)\n",
    "axes[0, 0].plot(Y_train_actual, label='Actual Sales', color='#2ca02c', linewidth=2, marker='o', markersize=3)\n",
    "axes[0, 0].plot(Y_train_pred_actual, label='Predicted Sales', color='#ff7f0e', linewidth=2, marker='s', markersize=3, alpha=0.7)\n",
    "axes[0, 0].set_title('Training Set: Actual vs Predicted Sales', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Sample Index')\n",
    "axes[0, 0].set_ylabel('Sales')\n",
    "axes[0, 0].legend(fontsize=10)\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Test vs Predicted (Test Set)\n",
    "axes[0, 1].plot(Y_test_actual, label='Actual Sales', color='#2ca02c', linewidth=2, marker='o', markersize=4)\n",
    "axes[0, 1].plot(Y_test_pred_actual, label='Predicted Sales', color='#d62728', linewidth=2, marker='s', markersize=4, alpha=0.7)\n",
    "axes[0, 1].set_title('Test Set: Actual vs Predicted Sales', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Sample Index')\n",
    "axes[0, 1].set_ylabel('Sales')\n",
    "axes[0, 1].legend(fontsize=10)\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Scatter Plot: Predicted vs Actual\n",
    "axes[1, 0].scatter(Y_test_actual, Y_test_pred_actual, alpha=0.6, color='#1f77b4', edgecolor='black', s=80)\n",
    "# Perfect prediction line\n",
    "min_val = min(Y_test_actual.min(), Y_test_pred_actual.min())\n",
    "max_val = max(Y_test_actual.max(), Y_test_pred_actual.max())\n",
    "axes[1, 0].plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect Prediction')\n",
    "axes[1, 0].set_title('Predicted vs Actual Sales (Test Set)', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Actual Sales')\n",
    "axes[1, 0].set_ylabel('Predicted Sales')\n",
    "axes[1, 0].legend(fontsize=10)\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Residual Plot\n",
    "residuals = Y_test_actual - Y_test_pred_actual\n",
    "axes[1, 1].scatter(Y_test_pred_actual, residuals, alpha=0.6, color='#9467bd', edgecolor='black', s=80)\n",
    "axes[1, 1].axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
    "axes[1, 1].set_title('Residual Plot (Test Set)', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Predicted Sales')\n",
    "axes[1, 1].set_ylabel('Residuals (Actual - Predicted)')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba3aa16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional Visualization: Complete Timeline\n",
    "fig, ax = plt.subplots(figsize=(16, 6))\n",
    "\n",
    "# Combine train and test for full timeline\n",
    "full_actual = np.concatenate([Y_train_actual, Y_test_actual])\n",
    "full_predicted = np.concatenate([Y_train_pred_actual, Y_test_pred_actual])\n",
    "\n",
    "# Plot complete timeline\n",
    "train_indices = range(len(Y_train_actual))\n",
    "test_indices = range(len(Y_train_actual), len(Y_train_actual) + len(Y_test_actual))\n",
    "\n",
    "ax.plot(train_indices, Y_train_actual, color='#2ca02c', linewidth=2, marker='o', markersize=4, label='Train Actual')\n",
    "ax.plot(train_indices, Y_train_pred_actual, color='#ff7f0e', linewidth=2, marker='s', markersize=4, alpha=0.7, label='Train Predicted')\n",
    "ax.plot(test_indices, Y_test_actual, color='#1f77b4', linewidth=2, marker='o', markersize=4, label='Test Actual')\n",
    "ax.plot(test_indices, Y_test_pred_actual, color='#d62728', linewidth=2, marker='s', markersize=4, alpha=0.7, label='Test Predicted')\n",
    "\n",
    "# Add vertical line to separate train and test\n",
    "ax.axvline(x=len(Y_train_actual)-0.5, color='black', linestyle='--', linewidth=2, label='Train/Test Split')\n",
    "\n",
    "ax.set_title('Complete Timeline: RNN Predictions on Book Sales', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Time Index')\n",
    "ax.set_ylabel('Paperback Sales')\n",
    "ax.legend(fontsize=10, loc='best')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f92952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance Metrics Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# 1. Metrics Bar Chart\n",
    "metrics = ['RMSE', 'MAE', 'MAPE']\n",
    "values = [rmse, mae, mape]\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c']\n",
    "\n",
    "bars = axes[0].bar(metrics, values, color=colors, alpha=0.7, edgecolor='black')\n",
    "axes[0].set_title('Error Metrics (Test Set)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Error Value')\n",
    "for i, v in enumerate(values):\n",
    "    axes[0].text(i, v + 1, f'{v:.2f}', ha='center', fontsize=11, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 2. R² Score Gauge\n",
    "axes[1].barh(['R² Score'], [r2], color='#d62728', alpha=0.7, edgecolor='black', height=0.3)\n",
    "axes[1].barh(['R² Score'], [1-r2], left=[r2], color='lightgray', alpha=0.5, edgecolor='black', height=0.3)\n",
    "axes[1].set_xlim([0, 1])\n",
    "axes[1].set_title('R² Score (Test Set)', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Score')\n",
    "axes[1].text(r2/2, 0, f'{r2:.4f}', ha='center', va='center', fontsize=14, fontweight='bold', color='white')\n",
    "axes[1].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n✓ Model Performance: R² = {r2:.4f}, RMSE = {rmse:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
